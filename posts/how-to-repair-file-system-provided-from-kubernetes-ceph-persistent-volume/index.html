<!doctype html><html lang=en-us><head><meta charset=utf-8><meta content="CSI,Ingress,Kubernetes" name=keywords><meta content="width=device-width,initial-scale=1" name=viewport><meta property="og:title" content="How to Repair File System Provided From Kubernetes Ceph Persistent Volume"><meta property="og:description" content="How to repair file system provided from Kubernetes ceph persistent volume 某日由於底層VM出現硬體故障，整座自建的Kubernetes接受影響， 部署於該環境的應用MySQL無法掛載pv，從pod出現的錯誤訊息如下:
MountVolume.SetUp failed for volume &#34;pvc-78cf22fa-d776-43a4-98d7-d594f02ea018&#34; : mount command failed, status: Failure, reason: failed to mount volume /dev/rbd13 [xfs] to /var/lib/kubelet/plugins/ceph.rook.io/rook-ceph/mounts/pvc-78cf22fa-d776-43a4-98d7-d594f02ea018, error mount failed: exit status 32 Mounting command: systemd-run Mounting arguments: --description=Kubernetes transient mount for /var/lib/kubelet/plugins/ceph.rook.io/rook-ceph/mounts/pvc-78cf22fa-d776-43a4-98d7-d594f02ea018 --scope -- mount -t xfs -o rw,defaults /dev/rbd13 /var/lib/kubelet/plugins/ceph.rook.io/rook-ceph/mounts/pvc-78cf22fa-d776-43a4-98d7-d594f02ea018 Output: Running scope as unit run-r2594d6c82152421c8891bfa8761e8c05.scope. mount: mount /dev/rbd13 on /var/lib/kubelet/plugins/ceph.rook.io/rook-ceph/mounts/pvc-78cf22fa-d776-43a4-98d7-d594f02ea018 failed: Structure needs cleaning 處理方式: 透過ceph tool重新掛載該pv的image，試著使用fsck修復"><meta property="og:type" content="article"><meta property="og:url" content="https://chienfuchen32.github.io/posts/how-to-repair-file-system-provided-from-kubernetes-ceph-persistent-volume/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-05-06T09:29:38+08:00"><meta property="article:modified_time" content="2023-05-06T09:29:38+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="How to Repair File System Provided From Kubernetes Ceph Persistent Volume"><meta name=twitter:description content="How to repair file system provided from Kubernetes ceph persistent volume 某日由於底層VM出現硬體故障，整座自建的Kubernetes接受影響， 部署於該環境的應用MySQL無法掛載pv，從pod出現的錯誤訊息如下:
MountVolume.SetUp failed for volume &#34;pvc-78cf22fa-d776-43a4-98d7-d594f02ea018&#34; : mount command failed, status: Failure, reason: failed to mount volume /dev/rbd13 [xfs] to /var/lib/kubelet/plugins/ceph.rook.io/rook-ceph/mounts/pvc-78cf22fa-d776-43a4-98d7-d594f02ea018, error mount failed: exit status 32 Mounting command: systemd-run Mounting arguments: --description=Kubernetes transient mount for /var/lib/kubelet/plugins/ceph.rook.io/rook-ceph/mounts/pvc-78cf22fa-d776-43a4-98d7-d594f02ea018 --scope -- mount -t xfs -o rw,defaults /dev/rbd13 /var/lib/kubelet/plugins/ceph.rook.io/rook-ceph/mounts/pvc-78cf22fa-d776-43a4-98d7-d594f02ea018 Output: Running scope as unit run-r2594d6c82152421c8891bfa8761e8c05.scope. mount: mount /dev/rbd13 on /var/lib/kubelet/plugins/ceph.rook.io/rook-ceph/mounts/pvc-78cf22fa-d776-43a4-98d7-d594f02ea018 failed: Structure needs cleaning 處理方式: 透過ceph tool重新掛載該pv的image，試著使用fsck修復"><title>How to Repair File System Provided From Kubernetes Ceph Persistent Volume &#183; Jeff’s note</title><link crossorigin=anonymous href=https://cdn.jsdelivr.net/npm/bootstrap@5.2.0/dist/css/bootstrap.min.css integrity=sha384-gH2yIJqKdNHPEq0n4Mqa/HGKIhSkIHeL5AyhkYV8i59U5AR6csBvApHHNl/vI1Bx rel=stylesheet><link crossorigin=anonymous href=https://cdn.jsdelivr.net/npm/bootstrap-icons@1.9.1/font/bootstrap-icons.css rel=stylesheet></head><body class="d-flex flex-column"><div class="container flex-fill"><header><div class=row><div class=col><nav class="d-flex justify-content-center my-3"><ul class="nav nav-pills"><li class=nav-item><a href=/about/ class=nav-link>About</a></li><li class=nav-item><a href=/posts/ class="nav-link active">Posts</a></li><li class=nav-item><a href=/tags/ class=nav-link>Tags</a></li></ul></nav></div></div></header><main><article><header><div class="justify-content-center row"><div class="col col-auto text-center"><div style=max-width:100ch><h1 class="display-5 fw-bold">How to Repair File System Provided From Kubernetes Ceph Persistent Volume</h1><p class=text-muted><time datetime=2023-05-06>May 6, 2023</time></p></div></div></div></header><div class="justify-content-center row"><div class="col col-auto"><div style=max-width:66ch><h1 id=how-to-repair-file-system-provided-from-kubernetes-ceph-persistent-volume>How to repair file system provided from Kubernetes ceph persistent volume</h1><p>某日由於底層VM出現硬體故障，整座自建的Kubernetes接受影響，
部署於該環境的應用MySQL無法掛載pv，從pod出現的錯誤訊息如下:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>MountVolume.SetUp failed <span style=color:#00f>for</span> volume <span style=color:#a31515>&#34;pvc-78cf22fa-d776-43a4-98d7-d594f02ea018&#34;</span> : mount command failed, status: Failure, reason: failed to mount volume /dev/rbd13 [xfs] to /var/lib/kubelet/plugins/ceph.rook.io/rook-ceph/mounts/pvc-78cf22fa-d776-43a4-98d7-d594f02ea018, error mount failed: exit status 32 Mounting command: systemd-run Mounting arguments: --description=Kubernetes transient mount <span style=color:#00f>for</span> /var/lib/kubelet/plugins/ceph.rook.io/rook-ceph/mounts/pvc-78cf22fa-d776-43a4-98d7-d594f02ea018 --scope -- mount -t xfs -o rw,defaults /dev/rbd13 /var/lib/kubelet/plugins/ceph.rook.io/rook-ceph/mounts/pvc-78cf22fa-d776-43a4-98d7-d594f02ea018 Output: Running scope as unit run-r2594d6c82152421c8891bfa8761e8c05.scope. mount: mount /dev/rbd13 on /var/lib/kubelet/plugins/ceph.rook.io/rook-ceph/mounts/pvc-78cf22fa-d776-43a4-98d7-d594f02ea018 failed: Structure needs cleaning
</span></span></code></pre></div><p>處理方式:
透過ceph tool重新掛載該pv的image，試著使用fsck修復</p><h2 id=詳細處理流程>詳細處理流程:</h2><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl exec -it -n rook-ceph rook-ceph-tools-69f996589-ps4k7 bash
</span></span><span style=display:flex><span>[root@twtpedev013 /]<span style=color:green># rbd map --pool=replicapool pvc-78cf22fa-d776-43a4-98d7-d594f02ea018</span>
</span></span><span style=display:flex><span>[root@twtpedev013 /]<span style=color:green># lsblk</span>
</span></span><span style=display:flex><span>NAME                                                                MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
</span></span><span style=display:flex><span>sda                                                                   8:0    0   16G  0 disk
</span></span><span style=display:flex><span>|-sda1                                                                8:1    0  731M  0 part
</span></span><span style=display:flex><span>|-sda2                                                                8:2    0    1K  0 part
</span></span><span style=display:flex><span><span style=color:#a31515>`</span>-sda5                                                                8:5    0 15.3G  0 part
</span></span><span style=display:flex><span>  |-ubuntu--vg-root                                                 252:0    0 14.3G  0 lvm  /etc/hosts
</span></span><span style=display:flex><span>  <span style=color:#a31515>`</span>-ubuntu--vg-swap_1                                               252:1    0  976M  0 lvm
</span></span><span style=display:flex><span>sdb                                                                   8:16   0 1000G  0 disk
</span></span><span style=display:flex><span><span style=color:#a31515>`</span>-ceph--4b83cf83--445d--42cc--8eeb--f7febf99fcab-osd--data--ee3fbf0e--4521--46fd--9879--e73048f7a697
</span></span><span style=display:flex><span>                                                                    252:2    0  999G  0 lvm
</span></span><span style=display:flex><span>sr0                                                                  11:0    1 1024M  0 rom
</span></span><span style=display:flex><span>rbd0                                                                251:0    0   10G  0 disk
</span></span><span style=display:flex><span>[root@twtpedev013 /]<span style=color:green># mkdir test-mysql</span>
</span></span><span style=display:flex><span>[root@twtpedev013 /]<span style=color:green># mount /dev/rbd0 test-mysql/</span>
</span></span><span style=display:flex><span>mount: mount /dev/rbd0 on /test-mysql failed: Structure needs cleaning
</span></span><span style=display:flex><span>[root@twtpedev013 /]<span style=color:green># fsck /dev/rbd0</span>
</span></span><span style=display:flex><span>fsck from util-linux 2.23.2
</span></span><span style=display:flex><span>If you wish to check the consistency of an XFS filesystem or
</span></span><span style=display:flex><span>repair a damaged filesystem, see xfs_repair(8).
</span></span><span style=display:flex><span>[root@twtpedev013 /]<span style=color:green># xfs_repair -L /dev/rbd0</span>
</span></span><span style=display:flex><span>Phase 1 - find and verify superblock...
</span></span><span style=display:flex><span>- reporting progress in intervals of 15 minutes
</span></span><span style=display:flex><span>Phase 2 - using internal log
</span></span><span style=display:flex><span>- zero log...
</span></span><span style=display:flex><span>ALERT: The filesystem has valuable metadata changes in a log which is being
</span></span><span style=display:flex><span>destroyed because the -L option was used.
</span></span><span style=display:flex><span>- scan filesystem freespace and inode maps...
</span></span><span style=display:flex><span>agi unlinked bucket 35 is 8227 in ag 0 (inode=8227)
</span></span><span style=display:flex><span>sb_ifree 228, counted 227
</span></span><span style=display:flex><span>sb_fdblocks 2398405, counted 2426486
</span></span><span style=display:flex><span>- 02:26:05: scanning filesystem freespace - 17 of 17 allocation groups <span style=color:#00f>done</span>
</span></span><span style=display:flex><span>- found root inode chunk
</span></span><span style=display:flex><span>Phase 3 - <span style=color:#00f>for</span> each AG...
</span></span><span style=display:flex><span>- scan and clear agi unlinked lists...
</span></span><span style=display:flex><span>- 02:26:05: scanning agi unlinked lists - 17 of 17 allocation groups <span style=color:#00f>done</span>
</span></span><span style=display:flex><span>- process known inodes and perform inode discovery...
</span></span><span style=display:flex><span>- agno = 0
</span></span><span style=display:flex><span>- agno = 15
</span></span><span style=display:flex><span>- agno = 16
</span></span><span style=display:flex><span>Metadata CRC error detected at xfs_bmbt block 0x2d9ff8/0x1000
</span></span><span style=display:flex><span>data fork in ino 8226 claims free block 1112
</span></span><span style=display:flex><span>correcting imap
</span></span><span style=display:flex><span>Metadata CRC error detected at xfs_bmbt block 0x2d9ff8/0x1000
</span></span><span style=display:flex><span>btree block 2/48127 is suspect, error -74
</span></span><span style=display:flex><span>bad magic <span style=color:green># 0 in inode 8227 (data fork) bmbt block 572415</span>
</span></span><span style=display:flex><span>bad data fork in inode 8227
</span></span><span style=display:flex><span>cleared inode 8227
</span></span><span style=display:flex><span>- agno = 1
</span></span><span style=display:flex><span>- agno = 2
</span></span><span style=display:flex><span>- agno = 3
</span></span><span style=display:flex><span>- agno = 4
</span></span><span style=display:flex><span>- agno = 5
</span></span><span style=display:flex><span>- agno = 6
</span></span><span style=display:flex><span>- agno = 7
</span></span><span style=display:flex><span>- agno = 8
</span></span><span style=display:flex><span>- agno = 9
</span></span><span style=display:flex><span>- agno = 10
</span></span><span style=display:flex><span>- agno = 11
</span></span><span style=display:flex><span>- agno = 12
</span></span><span style=display:flex><span>- agno = 13
</span></span><span style=display:flex><span>- agno = 14
</span></span><span style=display:flex><span>- 02:26:05: process known inodes and inode discovery - 384 of 384 inodes <span style=color:#00f>done</span>
</span></span><span style=display:flex><span>- process newly discovered inodes...
</span></span><span style=display:flex><span>- 02:26:05: process newly discovered inodes - 17 of 17 allocation groups <span style=color:#00f>done</span>
</span></span><span style=display:flex><span>Phase 4 - check <span style=color:#00f>for</span> duplicate blocks...
</span></span><span style=display:flex><span>- setting up duplicate extent list...
</span></span><span style=display:flex><span>- 02:26:05: setting up duplicate extent list - 17 of 17 allocation groups <span style=color:#00f>done</span>
</span></span><span style=display:flex><span>- check <span style=color:#00f>for</span> inodes claiming duplicate blocks...
</span></span><span style=display:flex><span>- agno = 0
</span></span><span style=display:flex><span>- agno = 6
</span></span><span style=display:flex><span>- agno = 7
</span></span><span style=display:flex><span>- agno = 8
</span></span><span style=display:flex><span>- agno = 9
</span></span><span style=display:flex><span>- agno = 10
</span></span><span style=display:flex><span>- agno = 11
</span></span><span style=display:flex><span>- agno = 12
</span></span><span style=display:flex><span>- agno = 13
</span></span><span style=display:flex><span>- agno = 14
</span></span><span style=display:flex><span>- agno = 15
</span></span><span style=display:flex><span>- agno = 16
</span></span><span style=display:flex><span>- agno = 1
</span></span><span style=display:flex><span>- agno = 3
</span></span><span style=display:flex><span>- agno = 4
</span></span><span style=display:flex><span>- agno = 5
</span></span><span style=display:flex><span>- agno = 2
</span></span><span style=display:flex><span>- 02:26:05: check <span style=color:#00f>for</span> inodes claiming duplicate blocks - 384 of 384 inodes <span style=color:#00f>done</span>
</span></span><span style=display:flex><span>Phase 5 - rebuild AG headers and trees...
</span></span><span style=display:flex><span>- 02:26:05: rebuild AG headers and trees - 17 of 17 allocation groups <span style=color:#00f>done</span>
</span></span><span style=display:flex><span>- reset superblock...
</span></span><span style=display:flex><span>Phase 6 - check inode connectivity...
</span></span><span style=display:flex><span>- resetting contents of realtime bitmap and summary inodes
</span></span><span style=display:flex><span>- traversing filesystem ...
</span></span><span style=display:flex><span>- traversal finished ...
</span></span><span style=display:flex><span>- moving disconnected inodes to lost+found ...
</span></span><span style=display:flex><span>Phase 7 - verify and correct link counts...
</span></span><span style=display:flex><span>- 02:26:05: verify and correct link counts - 17 of 17 allocation groups <span style=color:#00f>done</span>
</span></span><span style=display:flex><span>Metadata corruption detected at xfs_bmbt block 0x2d9ff8/0x1000
</span></span><span style=display:flex><span>libxfs_writebufr: write verifer failed on xfs_bmbt bno 0x2d9ff8/0x1000
</span></span><span style=display:flex><span>Maximum metadata LSN (82836:5824) is ahead of log (1:64).
</span></span><span style=display:flex><span>Format log to cycle 82839.
</span></span><span style=display:flex><span>releasing dirty buffer (bulk) to free list!done
</span></span><span style=display:flex><span>[root@twtpedev013 /]<span style=color:green># mount /dev/rbd0 test-mysql/</span>
</span></span><span style=display:flex><span>[root@twtpedev013 /]<span style=color:green># ls test-mysql/</span>
</span></span><span style=display:flex><span><span style=color:green>#ib_16384_0.dblwr binlog.000009 client-cert.pem ib_logfile1 performance_schema sys</span>
</span></span><span style=display:flex><span><span style=color:green>#ib_16384_1.dblwr binlog.index client-key.pem ibdata1 private_key.pem undo_001</span>
</span></span><span style=display:flex><span><span style=color:green>#innodb_temp binlog.~rec~ eams ibtmp1 public_key.pem undo_002</span>
</span></span><span style=display:flex><span>auto.cnf ca-key.pem ib_buffer_pool mysql server-cert.pem
</span></span><span style=display:flex><span>binlog.000008 ca.pem ib_logfile0 mysql.ibd server-key.pem
</span></span><span style=display:flex><span>[root@twtpedev013 /]<span style=color:green># umount /dev/rbd0</span>
</span></span><span style=display:flex><span>[root@twtpedev013 /]<span style=color:green># rbd unmap /dev/rbd0</span>
</span></span></code></pre></div><p>之後再重新將該MySQL deployment重啟發現正常了</p><pre tabindex=0><code>2023-05-04 10:28:38+08:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 8.0.21-1debian10 started.
2023-05-04 10:28:41+08:00 [Note] [Entrypoint]: Switching to dedicated user &#39;mysql&#39;
2023-05-04 10:28:41+08:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 8.0.21-1debian10 started.
2023-05-04T02:28:42.380318Z 0 [System] [MY-010116] [Server] /usr/sbin/mysqld (mysqld 8.0.21) starting as process 1
2023-05-04T02:28:42.428012Z 1 [System] [MY-013576] [InnoDB] InnoDB initialization has started.
2023-05-04T02:28:47.779944Z 1 [System] [MY-013577] [InnoDB] InnoDB initialization has ended.
2023-05-04T02:28:48.922746Z 0 [System] [MY-011323] [Server] X Plugin ready for connections. Bind-address: &#39;::&#39; port: 33060, socket: /var/run/mysqld/mysqlx.sock
2023-05-04T02:28:49.052930Z 0 [System] [MY-010229] [Server] Starting XA crash recovery...
2023-05-04T02:28:49.070234Z 0 [System] [MY-010232] [Server] XA crash recovery finished.
2023-05-04T02:28:49.287142Z 0 [Warning] [MY-010068] [Server] CA certificate ca.pem is self signed.
2023-05-04T02:28:49.287524Z 0 [System] [MY-013602] [Server] Channel mysql_main configured to support TLS. Encrypted connections are now supported for this channel.
2023-05-04T02:28:49.330334Z 0 [Warning] [MY-011810] [Server] Insecure configuration for --pid-file: Location &#39;/var/run/mysqld&#39; in the path is accessible to all OS users. Consider choosing a different directory.
2023-05-04T02:28:50.168132Z 0 [System] [MY-010931] [Server] /usr/sbin/mysqld: ready for connections. Version: &#39;8.0.21&#39;  socket: &#39;/var/run/mysqld/mysqld.sock&#39;  port: 3306  MySQL Community Server - GPL.
</code></pre><h2 id=ref>ref</h2><p><a href=https://docs.ceph.com/en/quincy/start/quick-rbd/>https://docs.ceph.com/en/quincy/start/quick-rbd/</a>
<a href=https://blog.51cto.com/u_13210651/2352686>https://blog.51cto.com/u_13210651/2352686</a>
<a href=https://www.tecmint.com/find-linux-filesystem-type/>https://www.tecmint.com/find-linux-filesystem-type/</a></p></div></div></div></article></main></div><script crossorigin=anonymous integrity=sha384-A3rJD856KowSb7dwlZdYEkO39Gagi7vIsF0jrRAoQmDKKtQBHUuLZ9AsSv4jD4Xa src=https://cdn.jsdelivr.net/npm/bootstrap@5.2.0/dist/js/bootstrap.bundle.min.js></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-3SNCDP141D"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-3SNCDP141D",{anonymize_ip:!1})}</script></body></html>