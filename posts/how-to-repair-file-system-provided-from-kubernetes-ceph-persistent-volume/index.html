<!doctype html><html lang=en-us><head><meta charset=utf-8><meta content="Jeff’s note" name=apple-mobile-web-app-title><meta content="CSI,Ingress,Kubernetes" name=keywords><meta content="#0d6efd" name=theme-color><meta content="width=device-width,initial-scale=1" name=viewport><meta property="og:url" content="https://chienfuchen32.github.io/posts/how-to-repair-file-system-provided-from-kubernetes-ceph-persistent-volume/"><meta property="og:site_name" content="Jeff's note"><meta property="og:title" content="How to Repair File System Provided From Kubernetes Ceph Persistent Volume"><meta property="og:description" content='How to repair file system provided from Kubernetes ceph persistent volume 某日由於底層VM出現硬體故障，整座自建的Kubernetes接受影響， 部署於該環境的應用MySQL無法掛載pv，從pod出現的錯誤訊息如下:
MountVolume.SetUp failed for volume "pvc-78cf22fa-d776-43a4-98d7-d594f02ea018" : mount command failed, status: Failure, reason: failed to mount volume /dev/rbd13 [xfs] to /var/lib/kubelet/plugins/ceph.rook.io/rook-ceph/mounts/pvc-78cf22fa-d776-43a4-98d7-d594f02ea018, error mount failed: exit status 32 Mounting command: systemd-run Mounting arguments: --description=Kubernetes transient mount for /var/lib/kubelet/plugins/ceph.rook.io/rook-ceph/mounts/pvc-78cf22fa-d776-43a4-98d7-d594f02ea018 --scope -- mount -t xfs -o rw,defaults /dev/rbd13 /var/lib/kubelet/plugins/ceph.rook.io/rook-ceph/mounts/pvc-78cf22fa-d776-43a4-98d7-d594f02ea018 Output: Running scope as unit run-r2594d6c82152421c8891bfa8761e8c05.scope. mount: mount /dev/rbd13 on /var/lib/kubelet/plugins/ceph.rook.io/rook-ceph/mounts/pvc-78cf22fa-d776-43a4-98d7-d594f02ea018 failed: Structure needs cleaning 處理方式: 透過ceph tool重新掛載該pv的image，試著使用fsck修復'><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-05-06T09:29:38+08:00"><meta property="article:modified_time" content="2023-05-06T09:29:38+08:00"><meta property="article:tag" content="Kubernetes"><meta property="article:tag" content="Ingress"><meta property="article:tag" content="CSI"><meta name=twitter:card content="summary"><meta name=twitter:title content="How to Repair File System Provided From Kubernetes Ceph Persistent Volume"><meta name=twitter:description content='How to repair file system provided from Kubernetes ceph persistent volume 某日由於底層VM出現硬體故障，整座自建的Kubernetes接受影響， 部署於該環境的應用MySQL無法掛載pv，從pod出現的錯誤訊息如下:
MountVolume.SetUp failed for volume "pvc-78cf22fa-d776-43a4-98d7-d594f02ea018" : mount command failed, status: Failure, reason: failed to mount volume /dev/rbd13 [xfs] to /var/lib/kubelet/plugins/ceph.rook.io/rook-ceph/mounts/pvc-78cf22fa-d776-43a4-98d7-d594f02ea018, error mount failed: exit status 32 Mounting command: systemd-run Mounting arguments: --description=Kubernetes transient mount for /var/lib/kubelet/plugins/ceph.rook.io/rook-ceph/mounts/pvc-78cf22fa-d776-43a4-98d7-d594f02ea018 --scope -- mount -t xfs -o rw,defaults /dev/rbd13 /var/lib/kubelet/plugins/ceph.rook.io/rook-ceph/mounts/pvc-78cf22fa-d776-43a4-98d7-d594f02ea018 Output: Running scope as unit run-r2594d6c82152421c8891bfa8761e8c05.scope. mount: mount /dev/rbd13 on /var/lib/kubelet/plugins/ceph.rook.io/rook-ceph/mounts/pvc-78cf22fa-d776-43a4-98d7-d594f02ea018 failed: Structure needs cleaning 處理方式: 透過ceph tool重新掛載該pv的image，試著使用fsck修復'><title>How to Repair File System Provided From Kubernetes Ceph Persistent Volume · Posts · Jeff’s note</title><link href=/apple-touch-icon.png rel=apple-touch-icon sizes=180x180><link href=/favicon-96x96.png rel=icon sizes=96x96 type=image/png><link href=/favicon.ico rel="shortcut icon"><link href=/favicon.svg rel=icon type=image/svg+xml><link href=/web-app-manifest-192x192.png rel=icon type=image/png><link href=/web-app-manifest-512x512.png rel=icon type=image/png><link href=/site.webmanifest rel=manifest><link crossorigin=anonymous href=/css/paige/paige.9bd1324054395b0ea2e122deb4382fd499298dd7.min.3b5b583ef4a5c7e93743e15582585c9c2a7621b8fef6c9c7ad4bac50e4781f05.css integrity="sha256-O1tYPvSlx+k3Q+FVglhcnCp2Ibj+9snHrUusUOR4HwU=" referrerpolicy=no-referrer rel=stylesheet><link crossorigin=anonymous href=/css/paige/bootstrap/paige.37b3fc66686a7163af8180c5fb0fc6ab835321eb.min.55516fbe90a5aad46d6dbc2441c1c193504ee32cb1eef5070ab589c776231a4c.css integrity="sha256-VVFvvpClqtRtbbwkQcHBk1BO4yyx7vUHCrWJx3YjGkw=" referrerpolicy=no-referrer rel=stylesheet><link crossorigin=anonymous href=/css/paige/bootstrap-icons/bootstrap-icons.min.755bbaf60ab33d8f6213c74d33daea2ef5bf22d15245142a27529f8c1d7e80f1.css integrity="sha256-dVu69gqzPY9iE8dNM9rqLvW/ItFSRRQqJ1KfjB1+gPE=" referrerpolicy=no-referrer rel=stylesheet></head><body><div class=container><div class=row><div class="col mt-3" id=paige-site><header id=paige-site-header><div class="display-1 fw-bold text-center" id=paige-site-title><a class="text-body text-decoration-none" href=/>Jeff&rsquo;s note</a></div><nav aria-label=Menu class=paige-row-tall id=paige-site-menu><ul class="align-items-center justify-content-center nav"><li class=nav-item><a class="nav-link text-decoration-underline" href=/about/>About</a></li><li class=nav-item><a aria-current=page class="active fw-bold text-body nav-link text-decoration-underline" href=/posts/>Posts</a></li><li class=nav-item><a class="nav-link text-decoration-underline" href=/tags/>Tags</a></li></ul></nav><nav aria-label=Breadcrumbs class=paige-row-tall id=paige-site-breadcrumbs><div class="d-flex justify-content-center"><ol class="breadcrumb mb-0"><li class=breadcrumb-item><a href=/>Home</a></li><li class=breadcrumb-item><a href=/posts/>Posts</a></li></ol></div></nav></header><article class="align-items-center d-flex flex-column paige-kind-page paige-status-draft paige-status-unpublished" id=paige-page><header class="mw-100 text-center" id=paige-page-header><h1 class=fw-bold id=paige-page-title>How to Repair File System Provided From Kubernetes Ceph Persistent Volume</h1><div><p class="paige-row-short text-secondary" id=paige-page-keywords><a class="link-secondary paige-page-keyword-tag" href=/tags/csi/>CSI</a>
·
<a class="link-secondary paige-page-keyword-tag" href=/tags/ingress/>Ingress</a>
·
<a class="link-secondary paige-page-keyword-tag" href=/tags/kubernetes/>Kubernetes</a></p><p class="paige-row-short text-secondary" id=paige-page-date><time datetime=2023-05-06>May 6, 2023</time></p><p class="paige-row-short text-secondary" id=paige-page-word-count>937 words</p><p class="paige-row-short text-secondary" id=paige-page-reading-time>5 minutes</p></div><div class="align-items-center d-flex flex-column paige-row-tall text-start" id=paige-page-toc><div class="border rounded" style="padding:1rem 2rem 0 1rem"><nav id=TableOfContents><ul><li><a href=#詳細處理流程>詳細處理流程:</a></li><li><a href=#ref>ref</a></li></ul></nav></div></div></header><main class=mw-100 id=paige-page-content><h1 class=h5 id=how-to-repair-file-system-provided-from-kubernetes-ceph-persistent-volume><a href=#how-to-repair-file-system-provided-from-kubernetes-ceph-persistent-volume style=color:rgba(var(--bs-body-color-rgb),var(--bs-text-opacity));text-decoration:none>How to repair file system provided from Kubernetes ceph persistent volume</a></h1><p>某日由於底層VM出現硬體故障，整座自建的Kubernetes接受影響，
部署於該環境的應用MySQL無法掛載pv，從pod出現的錯誤訊息如下:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>MountVolume.SetUp failed <span style=color:#00f>for</span> volume <span style=color:#a31515>&#34;pvc-78cf22fa-d776-43a4-98d7-d594f02ea018&#34;</span> : mount command failed, status: Failure, reason: failed to mount volume /dev/rbd13 [xfs] to /var/lib/kubelet/plugins/ceph.rook.io/rook-ceph/mounts/pvc-78cf22fa-d776-43a4-98d7-d594f02ea018, error mount failed: exit status 32 Mounting command: systemd-run Mounting arguments: --description=Kubernetes transient mount <span style=color:#00f>for</span> /var/lib/kubelet/plugins/ceph.rook.io/rook-ceph/mounts/pvc-78cf22fa-d776-43a4-98d7-d594f02ea018 --scope -- mount -t xfs -o rw,defaults /dev/rbd13 /var/lib/kubelet/plugins/ceph.rook.io/rook-ceph/mounts/pvc-78cf22fa-d776-43a4-98d7-d594f02ea018 Output: Running scope as unit run-r2594d6c82152421c8891bfa8761e8c05.scope. mount: mount /dev/rbd13 on /var/lib/kubelet/plugins/ceph.rook.io/rook-ceph/mounts/pvc-78cf22fa-d776-43a4-98d7-d594f02ea018 failed: Structure needs cleaning
</span></span></code></pre></div><p>處理方式:
透過ceph tool重新掛載該pv的image，試著使用fsck修復</p><h2 class=h6 id=詳細處理流程><a href=#%e8%a9%b3%e7%b4%b0%e8%99%95%e7%90%86%e6%b5%81%e7%a8%8b style=color:rgba(var(--bs-body-color-rgb),var(--bs-text-opacity));text-decoration:none>詳細處理流程:</a></h2><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl exec -it -n rook-ceph rook-ceph-tools-69f996589-ps4k7 bash
</span></span><span style=display:flex><span>[root@twtpedev013 /]<span style=color:green># rbd map --pool=replicapool pvc-78cf22fa-d776-43a4-98d7-d594f02ea018</span>
</span></span><span style=display:flex><span>[root@twtpedev013 /]<span style=color:green># lsblk</span>
</span></span><span style=display:flex><span>NAME                                                                MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
</span></span><span style=display:flex><span>sda                                                                   8:0    0   16G  0 disk
</span></span><span style=display:flex><span>|-sda1                                                                8:1    0  731M  0 part
</span></span><span style=display:flex><span>|-sda2                                                                8:2    0    1K  0 part
</span></span><span style=display:flex><span><span style=color:#a31515>`</span>-sda5                                                                8:5    0 15.3G  0 part
</span></span><span style=display:flex><span>  |-ubuntu--vg-root                                                 252:0    0 14.3G  0 lvm  /etc/hosts
</span></span><span style=display:flex><span>  <span style=color:#a31515>`</span>-ubuntu--vg-swap_1                                               252:1    0  976M  0 lvm
</span></span><span style=display:flex><span>sdb                                                                   8:16   0 1000G  0 disk
</span></span><span style=display:flex><span><span style=color:#a31515>`</span>-ceph--4b83cf83--445d--42cc--8eeb--f7febf99fcab-osd--data--ee3fbf0e--4521--46fd--9879--e73048f7a697
</span></span><span style=display:flex><span>                                                                    252:2    0  999G  0 lvm
</span></span><span style=display:flex><span>sr0                                                                  11:0    1 1024M  0 rom
</span></span><span style=display:flex><span>rbd0                                                                251:0    0   10G  0 disk
</span></span><span style=display:flex><span>[root@twtpedev013 /]<span style=color:green># mkdir test-mysql</span>
</span></span><span style=display:flex><span>[root@twtpedev013 /]<span style=color:green># mount /dev/rbd0 test-mysql/</span>
</span></span><span style=display:flex><span>mount: mount /dev/rbd0 on /test-mysql failed: Structure needs cleaning
</span></span><span style=display:flex><span>[root@twtpedev013 /]<span style=color:green># fsck /dev/rbd0</span>
</span></span><span style=display:flex><span>fsck from util-linux 2.23.2
</span></span><span style=display:flex><span>If you wish to check the consistency of an XFS filesystem or
</span></span><span style=display:flex><span>repair a damaged filesystem, see xfs_repair(8).
</span></span><span style=display:flex><span>[root@twtpedev013 /]<span style=color:green># xfs_repair -L /dev/rbd0</span>
</span></span><span style=display:flex><span>Phase 1 - find and verify superblock...
</span></span><span style=display:flex><span>- reporting progress in intervals of 15 minutes
</span></span><span style=display:flex><span>Phase 2 - using internal log
</span></span><span style=display:flex><span>- zero log...
</span></span><span style=display:flex><span>ALERT: The filesystem has valuable metadata changes in a log which is being
</span></span><span style=display:flex><span>destroyed because the -L option was used.
</span></span><span style=display:flex><span>- scan filesystem freespace and inode maps...
</span></span><span style=display:flex><span>agi unlinked bucket 35 is 8227 in ag 0 (inode=8227)
</span></span><span style=display:flex><span>sb_ifree 228, counted 227
</span></span><span style=display:flex><span>sb_fdblocks 2398405, counted 2426486
</span></span><span style=display:flex><span>- 02:26:05: scanning filesystem freespace - 17 of 17 allocation groups <span style=color:#00f>done</span>
</span></span><span style=display:flex><span>- found root inode chunk
</span></span><span style=display:flex><span>Phase 3 - <span style=color:#00f>for</span> each AG...
</span></span><span style=display:flex><span>- scan and clear agi unlinked lists...
</span></span><span style=display:flex><span>- 02:26:05: scanning agi unlinked lists - 17 of 17 allocation groups <span style=color:#00f>done</span>
</span></span><span style=display:flex><span>- process known inodes and perform inode discovery...
</span></span><span style=display:flex><span>- agno = 0
</span></span><span style=display:flex><span>- agno = 15
</span></span><span style=display:flex><span>- agno = 16
</span></span><span style=display:flex><span>Metadata CRC error detected at xfs_bmbt block 0x2d9ff8/0x1000
</span></span><span style=display:flex><span>data fork in ino 8226 claims free block 1112
</span></span><span style=display:flex><span>correcting imap
</span></span><span style=display:flex><span>Metadata CRC error detected at xfs_bmbt block 0x2d9ff8/0x1000
</span></span><span style=display:flex><span>btree block 2/48127 is suspect, error -74
</span></span><span style=display:flex><span>bad magic <span style=color:green># 0 in inode 8227 (data fork) bmbt block 572415</span>
</span></span><span style=display:flex><span>bad data fork in inode 8227
</span></span><span style=display:flex><span>cleared inode 8227
</span></span><span style=display:flex><span>- agno = 1
</span></span><span style=display:flex><span>- agno = 2
</span></span><span style=display:flex><span>- agno = 3
</span></span><span style=display:flex><span>- agno = 4
</span></span><span style=display:flex><span>- agno = 5
</span></span><span style=display:flex><span>- agno = 6
</span></span><span style=display:flex><span>- agno = 7
</span></span><span style=display:flex><span>- agno = 8
</span></span><span style=display:flex><span>- agno = 9
</span></span><span style=display:flex><span>- agno = 10
</span></span><span style=display:flex><span>- agno = 11
</span></span><span style=display:flex><span>- agno = 12
</span></span><span style=display:flex><span>- agno = 13
</span></span><span style=display:flex><span>- agno = 14
</span></span><span style=display:flex><span>- 02:26:05: process known inodes and inode discovery - 384 of 384 inodes <span style=color:#00f>done</span>
</span></span><span style=display:flex><span>- process newly discovered inodes...
</span></span><span style=display:flex><span>- 02:26:05: process newly discovered inodes - 17 of 17 allocation groups <span style=color:#00f>done</span>
</span></span><span style=display:flex><span>Phase 4 - check <span style=color:#00f>for</span> duplicate blocks...
</span></span><span style=display:flex><span>- setting up duplicate extent list...
</span></span><span style=display:flex><span>- 02:26:05: setting up duplicate extent list - 17 of 17 allocation groups <span style=color:#00f>done</span>
</span></span><span style=display:flex><span>- check <span style=color:#00f>for</span> inodes claiming duplicate blocks...
</span></span><span style=display:flex><span>- agno = 0
</span></span><span style=display:flex><span>- agno = 6
</span></span><span style=display:flex><span>- agno = 7
</span></span><span style=display:flex><span>- agno = 8
</span></span><span style=display:flex><span>- agno = 9
</span></span><span style=display:flex><span>- agno = 10
</span></span><span style=display:flex><span>- agno = 11
</span></span><span style=display:flex><span>- agno = 12
</span></span><span style=display:flex><span>- agno = 13
</span></span><span style=display:flex><span>- agno = 14
</span></span><span style=display:flex><span>- agno = 15
</span></span><span style=display:flex><span>- agno = 16
</span></span><span style=display:flex><span>- agno = 1
</span></span><span style=display:flex><span>- agno = 3
</span></span><span style=display:flex><span>- agno = 4
</span></span><span style=display:flex><span>- agno = 5
</span></span><span style=display:flex><span>- agno = 2
</span></span><span style=display:flex><span>- 02:26:05: check <span style=color:#00f>for</span> inodes claiming duplicate blocks - 384 of 384 inodes <span style=color:#00f>done</span>
</span></span><span style=display:flex><span>Phase 5 - rebuild AG headers and trees...
</span></span><span style=display:flex><span>- 02:26:05: rebuild AG headers and trees - 17 of 17 allocation groups <span style=color:#00f>done</span>
</span></span><span style=display:flex><span>- reset superblock...
</span></span><span style=display:flex><span>Phase 6 - check inode connectivity...
</span></span><span style=display:flex><span>- resetting contents of realtime bitmap and summary inodes
</span></span><span style=display:flex><span>- traversing filesystem ...
</span></span><span style=display:flex><span>- traversal finished ...
</span></span><span style=display:flex><span>- moving disconnected inodes to lost+found ...
</span></span><span style=display:flex><span>Phase 7 - verify and correct link counts...
</span></span><span style=display:flex><span>- 02:26:05: verify and correct link counts - 17 of 17 allocation groups <span style=color:#00f>done</span>
</span></span><span style=display:flex><span>Metadata corruption detected at xfs_bmbt block 0x2d9ff8/0x1000
</span></span><span style=display:flex><span>libxfs_writebufr: write verifer failed on xfs_bmbt bno 0x2d9ff8/0x1000
</span></span><span style=display:flex><span>Maximum metadata LSN (82836:5824) is ahead of log (1:64).
</span></span><span style=display:flex><span>Format log to cycle 82839.
</span></span><span style=display:flex><span>releasing dirty buffer (bulk) to free list!done
</span></span><span style=display:flex><span>[root@twtpedev013 /]<span style=color:green># mount /dev/rbd0 test-mysql/</span>
</span></span><span style=display:flex><span>[root@twtpedev013 /]<span style=color:green># ls test-mysql/</span>
</span></span><span style=display:flex><span><span style=color:green>#ib_16384_0.dblwr binlog.000009 client-cert.pem ib_logfile1 performance_schema sys</span>
</span></span><span style=display:flex><span><span style=color:green>#ib_16384_1.dblwr binlog.index client-key.pem ibdata1 private_key.pem undo_001</span>
</span></span><span style=display:flex><span><span style=color:green>#innodb_temp binlog.~rec~ eams ibtmp1 public_key.pem undo_002</span>
</span></span><span style=display:flex><span>auto.cnf ca-key.pem ib_buffer_pool mysql server-cert.pem
</span></span><span style=display:flex><span>binlog.000008 ca.pem ib_logfile0 mysql.ibd server-key.pem
</span></span><span style=display:flex><span>[root@twtpedev013 /]<span style=color:green># umount /dev/rbd0</span>
</span></span><span style=display:flex><span>[root@twtpedev013 /]<span style=color:green># rbd unmap /dev/rbd0</span>
</span></span></code></pre></div><p>之後再重新將該MySQL deployment重啟發現正常了</p><pre tabindex=0><code>2023-05-04 10:28:38+08:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 8.0.21-1debian10 started.
2023-05-04 10:28:41+08:00 [Note] [Entrypoint]: Switching to dedicated user &#39;mysql&#39;
2023-05-04 10:28:41+08:00 [Note] [Entrypoint]: Entrypoint script for MySQL Server 8.0.21-1debian10 started.
2023-05-04T02:28:42.380318Z 0 [System] [MY-010116] [Server] /usr/sbin/mysqld (mysqld 8.0.21) starting as process 1
2023-05-04T02:28:42.428012Z 1 [System] [MY-013576] [InnoDB] InnoDB initialization has started.
2023-05-04T02:28:47.779944Z 1 [System] [MY-013577] [InnoDB] InnoDB initialization has ended.
2023-05-04T02:28:48.922746Z 0 [System] [MY-011323] [Server] X Plugin ready for connections. Bind-address: &#39;::&#39; port: 33060, socket: /var/run/mysqld/mysqlx.sock
2023-05-04T02:28:49.052930Z 0 [System] [MY-010229] [Server] Starting XA crash recovery...
2023-05-04T02:28:49.070234Z 0 [System] [MY-010232] [Server] XA crash recovery finished.
2023-05-04T02:28:49.287142Z 0 [Warning] [MY-010068] [Server] CA certificate ca.pem is self signed.
2023-05-04T02:28:49.287524Z 0 [System] [MY-013602] [Server] Channel mysql_main configured to support TLS. Encrypted connections are now supported for this channel.
2023-05-04T02:28:49.330334Z 0 [Warning] [MY-011810] [Server] Insecure configuration for --pid-file: Location &#39;/var/run/mysqld&#39; in the path is accessible to all OS users. Consider choosing a different directory.
2023-05-04T02:28:50.168132Z 0 [System] [MY-010931] [Server] /usr/sbin/mysqld: ready for connections. Version: &#39;8.0.21&#39;  socket: &#39;/var/run/mysqld/mysqld.sock&#39;  port: 3306  MySQL Community Server - GPL.
</code></pre><h2 class=h6 id=ref><a href=#ref style=color:rgba(var(--bs-body-color-rgb),var(--bs-text-opacity));text-decoration:none>ref</a></h2><p><a href=https://docs.ceph.com/en/quincy/start/quick-rbd/ rel=external>https://docs.ceph.com/en/quincy/start/quick-rbd/</a>
<a href=https://blog.51cto.com/u_13210651/2352686 rel=external>https://blog.51cto.com/u_13210651/2352686</a>
<a href=https://www.tecmint.com/find-linux-filesystem-type/ rel=external>https://www.tecmint.com/find-linux-filesystem-type/</a></p></main><footer class=mw-100 id=paige-page-footer><div id=paige-page-siblings><div class="paige-row-short text-center text-secondary" id=paige-page-next><a class=link-secondary href=https://chienfuchen32.github.io/posts/continuous-integration-continuous-deployment/>Continuous Integration Continuous Deployment</a> &#8250;</div><div class="paige-row-short text-center text-secondary" id=paige-page-prev>&#8249; <a class=link-secondary href=https://chienfuchen32.github.io/posts/least-recently-used-cache/>Least Recently Used Cache</a></div></div></footer></article><footer id=paige-site-footer><p class="paige-row-short text-center text-secondary" id=paige-site-credit><a class="link-secondary text-decoration-none" href=https://github.com/willfaught/paige>Paige Theme</a></p></footer></div></div></div><script crossorigin=anonymous defer integrity="sha256-GdpV5fXTSprZ1he71RUKGZl0E3HtqxFXvbylEdkWmRQ=" referrerpolicy=no-referrer src=/js/paige/bootstrap/bootstrap.bundle.min.19da55e5f5d34a9ad9d617bbd5150a1999741371edab1157bdbca511d9169914.js></script><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","abstract":" How to repair file system provided from Kubernetes ceph persistent volume 某日由於底層VM出現硬體故障，整座自建的Kubernetes接受影響， 部署於該環境的應用MySQL無法掛載pv，從pod出現的錯誤訊息如下:\nMountVolume.SetUp failed for volume \"pvc-78cf22fa-d776-43a4-98d7-d594f02ea018\" : mount command failed, status: Failure, reason: failed to mount volume /dev/rbd13 [xfs] to /var/lib/kubelet/plugins/ceph.rook.io/rook-ceph/mounts/pvc-78cf22fa-d776-43a4-98d7-d594f02ea018, error mount failed: exit status 32 Mounting command: systemd-run Mounting arguments: --description=Kubernetes transient mount for /var/lib/kubelet/plugins/ceph.rook.io/rook-ceph/mounts/pvc-78cf22fa-d776-43a4-98d7-d594f02ea018 --scope -- mount -t xfs -o rw,defaults /dev/rbd13 /var/lib/kubelet/plugins/ceph.rook.io/rook-ceph/mounts/pvc-78cf22fa-d776-43a4-98d7-d594f02ea018 Output: Running scope as unit run-r2594d6c82152421c8891bfa8761e8c05.scope. mount: mount /dev/rbd13 on /var/lib/kubelet/plugins/ceph.rook.io/rook-ceph/mounts/pvc-78cf22fa-d776-43a4-98d7-d594f02ea018 failed: Structure needs cleaning 處理方式: 透過ceph tool重新掛載該pv的image，試著使用fsck修復","articleSection":"Posts","dateCreated":"2023-05-06T09:29:38+08:00","dateModified":"2023-05-06T09:29:38+08:00","headline":"How to Repair File System Provided From Kubernetes Ceph Persistent Volume","inLanguage":"en-us","keywords":"CSI, Ingress, Kubernetes","name":"How to Repair File System Provided From Kubernetes Ceph Persistent Volume","timeRequired":"PT5M","url":"https://chienfuchen32.github.io/posts/how-to-repair-file-system-provided-from-kubernetes-ceph-persistent-volume/","wordCount":937}</script><noscript>JavaScript is required</noscript></body></html>